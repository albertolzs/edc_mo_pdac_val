{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4615a83d-b9cd-4c1d-84df-d5e75c894c71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:01.387894429Z",
     "start_time": "2023-05-23T15:01:01.366280485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alberto/PycharmProjects/edc_mo_pdac_val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a073f0-ba9d-4545-a0f4-a49d638b7695",
   "metadata": {},
   "source": [
    "# Multi-omics stratification on PDAC patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7dffc4-d560-47bb-84b8-a6870d4824a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:03.420757620Z",
     "start_time": "2023-05-23T15:01:03.416255778Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "import time\n",
    "import dill\n",
    "import shutil\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from src import settings\n",
    "from src.optimization import Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57bbe66c-4bfc-400e-b091-d5c4e7470545",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "processed_data_folder = \"processed\"\n",
    "processed_data_path = os.path.join(data_folder, processed_data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991e212-7498-4da5-b089-c355594b1113",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b060443c-018b-4697-ad34-d04329ca9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(processed_data_path)\n",
    "files = list(filter(lambda x: os.path.isfile(os.path.join(processed_data_path, x)), files))\n",
    "file = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29bdb0b5-e40f-4046-a42b-d4f532bacc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = [StandardScaler().set_output(transform=\"pandas\").fit_transform(pd.read_csv(os.path.join(processed_data_path, filename), index_col=0)) for filename in files]\n",
    "samples = Xs[0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91209305-898a-4e6e-a8fd-137381938b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plasma_data.csv.shape (39, 663)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plasma_protein_A0M8Q6_LAC7</th>\n",
       "      <th>plasma_protein_B9A064_IGLL5</th>\n",
       "      <th>plasma_protein_O14791_APOL1</th>\n",
       "      <th>plasma_protein_O14980_XPO1</th>\n",
       "      <th>plasma_protein_O15212_PFD6</th>\n",
       "      <th>plasma_protein_O15230_LAMA5</th>\n",
       "      <th>plasma_protein_O43866_CD5L</th>\n",
       "      <th>plasma_protein_O75369_FLNB</th>\n",
       "      <th>plasma_protein_O75390_CISY</th>\n",
       "      <th>plasma_protein_O75882_ATRN</th>\n",
       "      <th>...</th>\n",
       "      <th>plasma_lipid_fatty_acid_comp_TAG(FA16:0)</th>\n",
       "      <th>plasma_lipid_fatty_acid_comp_TAG(FA16:1)</th>\n",
       "      <th>plasma_lipid_fatty_acid_comp_TAG(FA17:0)</th>\n",
       "      <th>plasma_lipid_fatty_acid_comp_TAG(FA18:0)</th>\n",
       "      <th>plasma_lipid_fatty_acid_comp_TAG(FA18:1)</th>\n",
       "      <th>plasma_lipid_fatty_acid_comp_TAG(FA18:2)</th>\n",
       "      <th>plasma_lipid_fatty_acid_comp_TAG(FA18:3)</th>\n",
       "      <th>plasma_lipid_fatty_acid_comp_TAG(FA20:4)</th>\n",
       "      <th>plasma_lipid_fatty_acid_comp_TAG(FA20:5)</th>\n",
       "      <th>plasma_lipid_fatty_acid_comp_TAG(FA22:6)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GI-19-025</th>\n",
       "      <td>5.020191</td>\n",
       "      <td>5.451051</td>\n",
       "      <td>5.305104</td>\n",
       "      <td>5.781695</td>\n",
       "      <td>4.622499</td>\n",
       "      <td>4.318202</td>\n",
       "      <td>5.379278</td>\n",
       "      <td>4.856250</td>\n",
       "      <td>5.164457</td>\n",
       "      <td>3.899591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>1.4852</td>\n",
       "      <td>0.3907</td>\n",
       "      <td>27.4427</td>\n",
       "      <td>2.6384</td>\n",
       "      <td>21.3749</td>\n",
       "      <td>0.0903</td>\n",
       "      <td>2.3547</td>\n",
       "      <td>0.7212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-17-537</th>\n",
       "      <td>5.215109</td>\n",
       "      <td>5.590096</td>\n",
       "      <td>5.403843</td>\n",
       "      <td>5.535984</td>\n",
       "      <td>4.622499</td>\n",
       "      <td>4.318202</td>\n",
       "      <td>5.672487</td>\n",
       "      <td>4.673977</td>\n",
       "      <td>3.828509</td>\n",
       "      <td>3.845567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3447</td>\n",
       "      <td>0.5042</td>\n",
       "      <td>2.8104</td>\n",
       "      <td>0.5411</td>\n",
       "      <td>26.7298</td>\n",
       "      <td>3.9978</td>\n",
       "      <td>21.4354</td>\n",
       "      <td>0.0946</td>\n",
       "      <td>2.2662</td>\n",
       "      <td>0.4359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-19-002</th>\n",
       "      <td>5.045229</td>\n",
       "      <td>5.370674</td>\n",
       "      <td>5.106038</td>\n",
       "      <td>5.457131</td>\n",
       "      <td>4.932033</td>\n",
       "      <td>4.953377</td>\n",
       "      <td>5.713943</td>\n",
       "      <td>4.856250</td>\n",
       "      <td>4.539673</td>\n",
       "      <td>3.723098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.4536</td>\n",
       "      <td>3.0685</td>\n",
       "      <td>0.4594</td>\n",
       "      <td>22.6095</td>\n",
       "      <td>4.4593</td>\n",
       "      <td>22.8729</td>\n",
       "      <td>0.1054</td>\n",
       "      <td>1.8585</td>\n",
       "      <td>1.3486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-16-575</th>\n",
       "      <td>5.151848</td>\n",
       "      <td>5.656317</td>\n",
       "      <td>5.294460</td>\n",
       "      <td>5.517482</td>\n",
       "      <td>4.969732</td>\n",
       "      <td>4.198990</td>\n",
       "      <td>5.371976</td>\n",
       "      <td>4.575006</td>\n",
       "      <td>4.539673</td>\n",
       "      <td>3.359389</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0218</td>\n",
       "      <td>2.3462</td>\n",
       "      <td>0.4330</td>\n",
       "      <td>3.6900</td>\n",
       "      <td>39.5313</td>\n",
       "      <td>24.9905</td>\n",
       "      <td>1.9591</td>\n",
       "      <td>1.3469</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>1.6169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-14-974</th>\n",
       "      <td>4.991846</td>\n",
       "      <td>5.427921</td>\n",
       "      <td>5.122799</td>\n",
       "      <td>5.303777</td>\n",
       "      <td>4.622499</td>\n",
       "      <td>4.400274</td>\n",
       "      <td>5.260241</td>\n",
       "      <td>4.812790</td>\n",
       "      <td>4.539673</td>\n",
       "      <td>3.706972</td>\n",
       "      <td>...</td>\n",
       "      <td>26.1365</td>\n",
       "      <td>3.0751</td>\n",
       "      <td>0.4406</td>\n",
       "      <td>2.6781</td>\n",
       "      <td>30.2909</td>\n",
       "      <td>27.8647</td>\n",
       "      <td>2.3295</td>\n",
       "      <td>2.4099</td>\n",
       "      <td>0.2761</td>\n",
       "      <td>1.3799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 663 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           plasma_protein_A0M8Q6_LAC7  plasma_protein_B9A064_IGLL5   \n",
       "GI-19-025                    5.020191                     5.451051  \\\n",
       "GI-17-537                    5.215109                     5.590096   \n",
       "GI-19-002                    5.045229                     5.370674   \n",
       "GI-16-575                    5.151848                     5.656317   \n",
       "GI-14-974                    4.991846                     5.427921   \n",
       "\n",
       "           plasma_protein_O14791_APOL1  plasma_protein_O14980_XPO1   \n",
       "GI-19-025                     5.305104                    5.781695  \\\n",
       "GI-17-537                     5.403843                    5.535984   \n",
       "GI-19-002                     5.106038                    5.457131   \n",
       "GI-16-575                     5.294460                    5.517482   \n",
       "GI-14-974                     5.122799                    5.303777   \n",
       "\n",
       "           plasma_protein_O15212_PFD6  plasma_protein_O15230_LAMA5   \n",
       "GI-19-025                    4.622499                     4.318202  \\\n",
       "GI-17-537                    4.622499                     4.318202   \n",
       "GI-19-002                    4.932033                     4.953377   \n",
       "GI-16-575                    4.969732                     4.198990   \n",
       "GI-14-974                    4.622499                     4.400274   \n",
       "\n",
       "           plasma_protein_O43866_CD5L  plasma_protein_O75369_FLNB   \n",
       "GI-19-025                    5.379278                    4.856250  \\\n",
       "GI-17-537                    5.672487                    4.673977   \n",
       "GI-19-002                    5.713943                    4.856250   \n",
       "GI-16-575                    5.371976                    4.575006   \n",
       "GI-14-974                    5.260241                    4.812790   \n",
       "\n",
       "           plasma_protein_O75390_CISY  plasma_protein_O75882_ATRN  ...   \n",
       "GI-19-025                    5.164457                    3.899591  ...  \\\n",
       "GI-17-537                    3.828509                    3.845567  ...   \n",
       "GI-19-002                    4.539673                    3.723098  ...   \n",
       "GI-16-575                    4.539673                    3.359389  ...   \n",
       "GI-14-974                    4.539673                    3.706972  ...   \n",
       "\n",
       "           plasma_lipid_fatty_acid_comp_TAG(FA16:0)   \n",
       "GI-19-025                                    0.0842  \\\n",
       "GI-17-537                                    0.3447   \n",
       "GI-19-002                                    0.1580   \n",
       "GI-16-575                                   21.0218   \n",
       "GI-14-974                                   26.1365   \n",
       "\n",
       "           plasma_lipid_fatty_acid_comp_TAG(FA16:1)   \n",
       "GI-19-025                                    0.2654  \\\n",
       "GI-17-537                                    0.5042   \n",
       "GI-19-002                                    0.4536   \n",
       "GI-16-575                                    2.3462   \n",
       "GI-14-974                                    3.0751   \n",
       "\n",
       "           plasma_lipid_fatty_acid_comp_TAG(FA17:0)   \n",
       "GI-19-025                                    1.4852  \\\n",
       "GI-17-537                                    2.8104   \n",
       "GI-19-002                                    3.0685   \n",
       "GI-16-575                                    0.4330   \n",
       "GI-14-974                                    0.4406   \n",
       "\n",
       "           plasma_lipid_fatty_acid_comp_TAG(FA18:0)   \n",
       "GI-19-025                                    0.3907  \\\n",
       "GI-17-537                                    0.5411   \n",
       "GI-19-002                                    0.4594   \n",
       "GI-16-575                                    3.6900   \n",
       "GI-14-974                                    2.6781   \n",
       "\n",
       "           plasma_lipid_fatty_acid_comp_TAG(FA18:1)   \n",
       "GI-19-025                                   27.4427  \\\n",
       "GI-17-537                                   26.7298   \n",
       "GI-19-002                                   22.6095   \n",
       "GI-16-575                                   39.5313   \n",
       "GI-14-974                                   30.2909   \n",
       "\n",
       "           plasma_lipid_fatty_acid_comp_TAG(FA18:2)   \n",
       "GI-19-025                                    2.6384  \\\n",
       "GI-17-537                                    3.9978   \n",
       "GI-19-002                                    4.4593   \n",
       "GI-16-575                                   24.9905   \n",
       "GI-14-974                                   27.8647   \n",
       "\n",
       "           plasma_lipid_fatty_acid_comp_TAG(FA18:3)   \n",
       "GI-19-025                                   21.3749  \\\n",
       "GI-17-537                                   21.4354   \n",
       "GI-19-002                                   22.8729   \n",
       "GI-16-575                                    1.9591   \n",
       "GI-14-974                                    2.3295   \n",
       "\n",
       "           plasma_lipid_fatty_acid_comp_TAG(FA20:4)   \n",
       "GI-19-025                                    0.0903  \\\n",
       "GI-17-537                                    0.0946   \n",
       "GI-19-002                                    0.1054   \n",
       "GI-16-575                                    1.3469   \n",
       "GI-14-974                                    2.4099   \n",
       "\n",
       "           plasma_lipid_fatty_acid_comp_TAG(FA20:5)   \n",
       "GI-19-025                                    2.3547  \\\n",
       "GI-17-537                                    2.2662   \n",
       "GI-19-002                                    1.8585   \n",
       "GI-16-575                                    0.4768   \n",
       "GI-14-974                                    0.2761   \n",
       "\n",
       "           plasma_lipid_fatty_acid_comp_TAG(FA22:6)  \n",
       "GI-19-025                                    0.7212  \n",
       "GI-17-537                                    0.4359  \n",
       "GI-19-002                                    1.3486  \n",
       "GI-16-575                                    1.6169  \n",
       "GI-14-974                                    1.3799  \n",
       "\n",
       "[5 rows x 663 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file += 1\n",
    "filename = files[file]\n",
    "data = pd.read_csv(os.path.join(processed_data_path, filename), index_col=0)\n",
    "print(f\"{filename}.shape\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71279cfb-ec7a-48a3-94a1-3c587c580a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tissue_data.csv.shape (39, 1130)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tissue_protein_A0A075B6S5_KV127</th>\n",
       "      <th>tissue_protein_A0A0C4DH35_HV335</th>\n",
       "      <th>tissue_protein_A0A0C4DH36_HV338</th>\n",
       "      <th>tissue_protein_A0AV96_RBM47</th>\n",
       "      <th>tissue_protein_A0FGR8_ESYT2</th>\n",
       "      <th>tissue_protein_A0MZ66_SHOT1</th>\n",
       "      <th>tissue_protein_A1X283_SPD2B</th>\n",
       "      <th>tissue_protein_A6NMY6_AXA2L</th>\n",
       "      <th>tissue_protein_A6NNZ2_TBB8B</th>\n",
       "      <th>tissue_protein_O00154_BACH</th>\n",
       "      <th>...</th>\n",
       "      <th>tissue_protein_Q9Y450_HBS1L</th>\n",
       "      <th>tissue_protein_Q9Y4E8_UBP15</th>\n",
       "      <th>tissue_protein_Q9Y5S9_RBM8A</th>\n",
       "      <th>tissue_protein_Q9Y5Y2_NUBP2</th>\n",
       "      <th>tissue_protein_Q9Y678_COPG1</th>\n",
       "      <th>tissue_protein_Q9Y6A9_SPCS1</th>\n",
       "      <th>tissue_protein_Q9Y6B6_SAR1B</th>\n",
       "      <th>tissue_protein_Q9Y6C2_EMIL1</th>\n",
       "      <th>tissue_protein_Q9Y6M9_NDUB9</th>\n",
       "      <th>tissue_protein_Q9Y6Y8_S23IP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GI-19-025</th>\n",
       "      <td>4.713994</td>\n",
       "      <td>4.233653</td>\n",
       "      <td>4.298054</td>\n",
       "      <td>3.697783</td>\n",
       "      <td>4.263094</td>\n",
       "      <td>3.460593</td>\n",
       "      <td>4.130536</td>\n",
       "      <td>3.517030</td>\n",
       "      <td>4.495384</td>\n",
       "      <td>3.881753</td>\n",
       "      <td>...</td>\n",
       "      <td>3.47681</td>\n",
       "      <td>4.612968</td>\n",
       "      <td>4.395142</td>\n",
       "      <td>3.398342</td>\n",
       "      <td>3.801523</td>\n",
       "      <td>4.395557</td>\n",
       "      <td>4.195019</td>\n",
       "      <td>4.883127</td>\n",
       "      <td>3.837916</td>\n",
       "      <td>4.260869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-17-537</th>\n",
       "      <td>4.713994</td>\n",
       "      <td>4.233653</td>\n",
       "      <td>4.298054</td>\n",
       "      <td>3.853952</td>\n",
       "      <td>4.366608</td>\n",
       "      <td>3.460593</td>\n",
       "      <td>3.643790</td>\n",
       "      <td>3.517030</td>\n",
       "      <td>4.495384</td>\n",
       "      <td>3.881753</td>\n",
       "      <td>...</td>\n",
       "      <td>3.47681</td>\n",
       "      <td>5.794042</td>\n",
       "      <td>4.432392</td>\n",
       "      <td>3.398342</td>\n",
       "      <td>5.252542</td>\n",
       "      <td>4.395557</td>\n",
       "      <td>3.782099</td>\n",
       "      <td>4.262695</td>\n",
       "      <td>4.683767</td>\n",
       "      <td>4.831078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-19-002</th>\n",
       "      <td>4.713994</td>\n",
       "      <td>4.640927</td>\n",
       "      <td>4.366604</td>\n",
       "      <td>3.697783</td>\n",
       "      <td>4.263094</td>\n",
       "      <td>3.460593</td>\n",
       "      <td>3.643790</td>\n",
       "      <td>3.517030</td>\n",
       "      <td>4.495384</td>\n",
       "      <td>3.881753</td>\n",
       "      <td>...</td>\n",
       "      <td>3.47681</td>\n",
       "      <td>4.612968</td>\n",
       "      <td>4.687704</td>\n",
       "      <td>3.398342</td>\n",
       "      <td>3.665189</td>\n",
       "      <td>4.395557</td>\n",
       "      <td>4.195019</td>\n",
       "      <td>5.625306</td>\n",
       "      <td>4.161362</td>\n",
       "      <td>4.260869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-16-575</th>\n",
       "      <td>5.050063</td>\n",
       "      <td>4.304040</td>\n",
       "      <td>4.293424</td>\n",
       "      <td>3.697783</td>\n",
       "      <td>4.263094</td>\n",
       "      <td>3.460593</td>\n",
       "      <td>3.643790</td>\n",
       "      <td>4.095981</td>\n",
       "      <td>4.940837</td>\n",
       "      <td>4.364585</td>\n",
       "      <td>...</td>\n",
       "      <td>3.47681</td>\n",
       "      <td>4.612968</td>\n",
       "      <td>4.395142</td>\n",
       "      <td>3.398342</td>\n",
       "      <td>5.518563</td>\n",
       "      <td>4.277215</td>\n",
       "      <td>4.381231</td>\n",
       "      <td>6.193125</td>\n",
       "      <td>4.806804</td>\n",
       "      <td>3.985681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-14-974</th>\n",
       "      <td>4.383966</td>\n",
       "      <td>4.039878</td>\n",
       "      <td>4.298054</td>\n",
       "      <td>3.865179</td>\n",
       "      <td>4.686643</td>\n",
       "      <td>3.460593</td>\n",
       "      <td>3.643790</td>\n",
       "      <td>4.175468</td>\n",
       "      <td>5.024638</td>\n",
       "      <td>4.694124</td>\n",
       "      <td>...</td>\n",
       "      <td>3.47681</td>\n",
       "      <td>4.612968</td>\n",
       "      <td>4.395142</td>\n",
       "      <td>3.398342</td>\n",
       "      <td>5.912920</td>\n",
       "      <td>4.394835</td>\n",
       "      <td>5.035698</td>\n",
       "      <td>5.979105</td>\n",
       "      <td>5.016900</td>\n",
       "      <td>3.781480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tissue_protein_A0A075B6S5_KV127  tissue_protein_A0A0C4DH35_HV335   \n",
       "GI-19-025                         4.713994                         4.233653  \\\n",
       "GI-17-537                         4.713994                         4.233653   \n",
       "GI-19-002                         4.713994                         4.640927   \n",
       "GI-16-575                         5.050063                         4.304040   \n",
       "GI-14-974                         4.383966                         4.039878   \n",
       "\n",
       "           tissue_protein_A0A0C4DH36_HV338  tissue_protein_A0AV96_RBM47   \n",
       "GI-19-025                         4.298054                     3.697783  \\\n",
       "GI-17-537                         4.298054                     3.853952   \n",
       "GI-19-002                         4.366604                     3.697783   \n",
       "GI-16-575                         4.293424                     3.697783   \n",
       "GI-14-974                         4.298054                     3.865179   \n",
       "\n",
       "           tissue_protein_A0FGR8_ESYT2  tissue_protein_A0MZ66_SHOT1   \n",
       "GI-19-025                     4.263094                     3.460593  \\\n",
       "GI-17-537                     4.366608                     3.460593   \n",
       "GI-19-002                     4.263094                     3.460593   \n",
       "GI-16-575                     4.263094                     3.460593   \n",
       "GI-14-974                     4.686643                     3.460593   \n",
       "\n",
       "           tissue_protein_A1X283_SPD2B  tissue_protein_A6NMY6_AXA2L   \n",
       "GI-19-025                     4.130536                     3.517030  \\\n",
       "GI-17-537                     3.643790                     3.517030   \n",
       "GI-19-002                     3.643790                     3.517030   \n",
       "GI-16-575                     3.643790                     4.095981   \n",
       "GI-14-974                     3.643790                     4.175468   \n",
       "\n",
       "           tissue_protein_A6NNZ2_TBB8B  tissue_protein_O00154_BACH  ...   \n",
       "GI-19-025                     4.495384                    3.881753  ...  \\\n",
       "GI-17-537                     4.495384                    3.881753  ...   \n",
       "GI-19-002                     4.495384                    3.881753  ...   \n",
       "GI-16-575                     4.940837                    4.364585  ...   \n",
       "GI-14-974                     5.024638                    4.694124  ...   \n",
       "\n",
       "           tissue_protein_Q9Y450_HBS1L  tissue_protein_Q9Y4E8_UBP15   \n",
       "GI-19-025                      3.47681                     4.612968  \\\n",
       "GI-17-537                      3.47681                     5.794042   \n",
       "GI-19-002                      3.47681                     4.612968   \n",
       "GI-16-575                      3.47681                     4.612968   \n",
       "GI-14-974                      3.47681                     4.612968   \n",
       "\n",
       "           tissue_protein_Q9Y5S9_RBM8A  tissue_protein_Q9Y5Y2_NUBP2   \n",
       "GI-19-025                     4.395142                     3.398342  \\\n",
       "GI-17-537                     4.432392                     3.398342   \n",
       "GI-19-002                     4.687704                     3.398342   \n",
       "GI-16-575                     4.395142                     3.398342   \n",
       "GI-14-974                     4.395142                     3.398342   \n",
       "\n",
       "           tissue_protein_Q9Y678_COPG1  tissue_protein_Q9Y6A9_SPCS1   \n",
       "GI-19-025                     3.801523                     4.395557  \\\n",
       "GI-17-537                     5.252542                     4.395557   \n",
       "GI-19-002                     3.665189                     4.395557   \n",
       "GI-16-575                     5.518563                     4.277215   \n",
       "GI-14-974                     5.912920                     4.394835   \n",
       "\n",
       "           tissue_protein_Q9Y6B6_SAR1B  tissue_protein_Q9Y6C2_EMIL1   \n",
       "GI-19-025                     4.195019                     4.883127  \\\n",
       "GI-17-537                     3.782099                     4.262695   \n",
       "GI-19-002                     4.195019                     5.625306   \n",
       "GI-16-575                     4.381231                     6.193125   \n",
       "GI-14-974                     5.035698                     5.979105   \n",
       "\n",
       "           tissue_protein_Q9Y6M9_NDUB9  tissue_protein_Q9Y6Y8_S23IP  \n",
       "GI-19-025                     3.837916                     4.260869  \n",
       "GI-17-537                     4.683767                     4.831078  \n",
       "GI-19-002                     4.161362                     4.260869  \n",
       "GI-16-575                     4.806804                     3.985681  \n",
       "GI-14-974                     5.016900                     3.781480  \n",
       "\n",
       "[5 rows x 1130 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file += 1\n",
    "filename = files[file]\n",
    "data = pd.read_csv(os.path.join(processed_data_path, filename), index_col=0)\n",
    "print(f\"{filename}.shape\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f62915-0113-4d62-ac2d-aa54f78b458e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pathology_data.csv.shape (39, 820)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pathology_NF40_25%</th>\n",
       "      <th>pathology_NF39_0.01</th>\n",
       "      <th>pathology_NF62_75%</th>\n",
       "      <th>pathology_NF58_0.01</th>\n",
       "      <th>pathology_NF46_0.1</th>\n",
       "      <th>pathology_NF16_0.95</th>\n",
       "      <th>pathology_NF59_min</th>\n",
       "      <th>pathology_NF22_std</th>\n",
       "      <th>pathology_NF37_75%</th>\n",
       "      <th>pathology_NF20_0.05</th>\n",
       "      <th>...</th>\n",
       "      <th>pathology_NF4_50%</th>\n",
       "      <th>pathology_NF63_max</th>\n",
       "      <th>pathology_NF49_50%</th>\n",
       "      <th>pathology_NF32_0.95</th>\n",
       "      <th>pathology_NF15_0.1</th>\n",
       "      <th>pathology_NF26_0.1</th>\n",
       "      <th>pathology_NF6_25%</th>\n",
       "      <th>pathology_NF28_0.1</th>\n",
       "      <th>pathology_NF5_50%</th>\n",
       "      <th>pathology_NF3_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GI-19-025</th>\n",
       "      <td>18.221364</td>\n",
       "      <td>0.083023</td>\n",
       "      <td>31.048349</td>\n",
       "      <td>0.884340</td>\n",
       "      <td>16971.053546</td>\n",
       "      <td>0.919779</td>\n",
       "      <td>0.165546</td>\n",
       "      <td>0.224469</td>\n",
       "      <td>3293.252206</td>\n",
       "      <td>363.037625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806650</td>\n",
       "      <td>361.342423</td>\n",
       "      <td>0.254049</td>\n",
       "      <td>0.023666</td>\n",
       "      <td>29.383830</td>\n",
       "      <td>0.948465</td>\n",
       "      <td>383.00</td>\n",
       "      <td>848.719310</td>\n",
       "      <td>0.591029</td>\n",
       "      <td>25.556694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-17-537</th>\n",
       "      <td>17.700665</td>\n",
       "      <td>0.084998</td>\n",
       "      <td>39.661064</td>\n",
       "      <td>0.663660</td>\n",
       "      <td>14967.563271</td>\n",
       "      <td>0.911952</td>\n",
       "      <td>0.165851</td>\n",
       "      <td>0.254959</td>\n",
       "      <td>3411.797462</td>\n",
       "      <td>339.028141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.703743</td>\n",
       "      <td>407.634914</td>\n",
       "      <td>0.257888</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>32.485244</td>\n",
       "      <td>0.936644</td>\n",
       "      <td>460.00</td>\n",
       "      <td>756.448280</td>\n",
       "      <td>0.710455</td>\n",
       "      <td>28.042728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-19-002</th>\n",
       "      <td>17.255396</td>\n",
       "      <td>0.085569</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.769103</td>\n",
       "      <td>16708.054344</td>\n",
       "      <td>0.908448</td>\n",
       "      <td>0.166843</td>\n",
       "      <td>0.214322</td>\n",
       "      <td>3415.990272</td>\n",
       "      <td>362.079941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791136</td>\n",
       "      <td>315.220854</td>\n",
       "      <td>0.260066</td>\n",
       "      <td>0.018552</td>\n",
       "      <td>33.917936</td>\n",
       "      <td>0.942633</td>\n",
       "      <td>390.00</td>\n",
       "      <td>898.706989</td>\n",
       "      <td>0.611640</td>\n",
       "      <td>25.718516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-16-575</th>\n",
       "      <td>17.291761</td>\n",
       "      <td>0.085349</td>\n",
       "      <td>33.615473</td>\n",
       "      <td>0.679883</td>\n",
       "      <td>15951.062804</td>\n",
       "      <td>0.905980</td>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.229177</td>\n",
       "      <td>3411.205173</td>\n",
       "      <td>360.440383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781992</td>\n",
       "      <td>450.018046</td>\n",
       "      <td>0.259194</td>\n",
       "      <td>0.019614</td>\n",
       "      <td>33.828096</td>\n",
       "      <td>0.943830</td>\n",
       "      <td>387.00</td>\n",
       "      <td>842.917498</td>\n",
       "      <td>0.623288</td>\n",
       "      <td>25.909778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-14-974</th>\n",
       "      <td>18.739805</td>\n",
       "      <td>0.083579</td>\n",
       "      <td>110.367568</td>\n",
       "      <td>0.850576</td>\n",
       "      <td>15711.336968</td>\n",
       "      <td>0.934820</td>\n",
       "      <td>0.189802</td>\n",
       "      <td>0.193320</td>\n",
       "      <td>2678.358771</td>\n",
       "      <td>369.434757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745780</td>\n",
       "      <td>722.832459</td>\n",
       "      <td>0.247976</td>\n",
       "      <td>0.019650</td>\n",
       "      <td>26.712580</td>\n",
       "      <td>0.952773</td>\n",
       "      <td>339.75</td>\n",
       "      <td>804.602788</td>\n",
       "      <td>0.666186</td>\n",
       "      <td>24.707599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 820 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pathology_NF40_25%  pathology_NF39_0.01  pathology_NF62_75%   \n",
       "GI-19-025           18.221364             0.083023           31.048349  \\\n",
       "GI-17-537           17.700665             0.084998           39.661064   \n",
       "GI-19-002           17.255396             0.085569           32.000000   \n",
       "GI-16-575           17.291761             0.085349           33.615473   \n",
       "GI-14-974           18.739805             0.083579          110.367568   \n",
       "\n",
       "           pathology_NF58_0.01  pathology_NF46_0.1  pathology_NF16_0.95   \n",
       "GI-19-025             0.884340        16971.053546             0.919779  \\\n",
       "GI-17-537             0.663660        14967.563271             0.911952   \n",
       "GI-19-002             0.769103        16708.054344             0.908448   \n",
       "GI-16-575             0.679883        15951.062804             0.905980   \n",
       "GI-14-974             0.850576        15711.336968             0.934820   \n",
       "\n",
       "           pathology_NF59_min  pathology_NF22_std  pathology_NF37_75%   \n",
       "GI-19-025            0.165546            0.224469         3293.252206  \\\n",
       "GI-17-537            0.165851            0.254959         3411.797462   \n",
       "GI-19-002            0.166843            0.214322         3415.990272   \n",
       "GI-16-575            0.164000            0.229177         3411.205173   \n",
       "GI-14-974            0.189802            0.193320         2678.358771   \n",
       "\n",
       "           pathology_NF20_0.05  ...  pathology_NF4_50%  pathology_NF63_max   \n",
       "GI-19-025           363.037625  ...           0.806650          361.342423  \\\n",
       "GI-17-537           339.028141  ...           0.703743          407.634914   \n",
       "GI-19-002           362.079941  ...           0.791136          315.220854   \n",
       "GI-16-575           360.440383  ...           0.781992          450.018046   \n",
       "GI-14-974           369.434757  ...           0.745780          722.832459   \n",
       "\n",
       "           pathology_NF49_50%  pathology_NF32_0.95  pathology_NF15_0.1   \n",
       "GI-19-025            0.254049             0.023666           29.383830  \\\n",
       "GI-17-537            0.257888             0.024600           32.485244   \n",
       "GI-19-002            0.260066             0.018552           33.917936   \n",
       "GI-16-575            0.259194             0.019614           33.828096   \n",
       "GI-14-974            0.247976             0.019650           26.712580   \n",
       "\n",
       "           pathology_NF26_0.1  pathology_NF6_25%  pathology_NF28_0.1   \n",
       "GI-19-025            0.948465             383.00          848.719310  \\\n",
       "GI-17-537            0.936644             460.00          756.448280   \n",
       "GI-19-002            0.942633             390.00          898.706989   \n",
       "GI-16-575            0.943830             387.00          842.917498   \n",
       "GI-14-974            0.952773             339.75          804.602788   \n",
       "\n",
       "           pathology_NF5_50%  pathology_NF3_mean  \n",
       "GI-19-025           0.591029           25.556694  \n",
       "GI-17-537           0.710455           28.042728  \n",
       "GI-19-002           0.611640           25.718516  \n",
       "GI-16-575           0.623288           25.909778  \n",
       "GI-14-974           0.666186           24.707599  \n",
       "\n",
       "[5 rows x 820 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file += 1\n",
    "filename = files[file]\n",
    "data = pd.read_csv(os.path.join(processed_data_path, filename), index_col=0)\n",
    "print(f\"{filename}.shape\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0042c4f6-dabc-45be-8d5b-69118f004391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNV_data.csv.shape (39, 648)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNV_HLA-DPB1</th>\n",
       "      <th>CNV_GPC3</th>\n",
       "      <th>CNV_SOX2</th>\n",
       "      <th>CNV_XRCC3</th>\n",
       "      <th>CNV_PAK1</th>\n",
       "      <th>CNV_ACVR1B</th>\n",
       "      <th>CNV_DDR2</th>\n",
       "      <th>CNV_FANCL</th>\n",
       "      <th>CNV_CDKN1B</th>\n",
       "      <th>CNV_DYNC2H1</th>\n",
       "      <th>...</th>\n",
       "      <th>CNV_NBN</th>\n",
       "      <th>CNV_HLA-DQB2</th>\n",
       "      <th>CNV_BRCA2</th>\n",
       "      <th>CNV_RAD51C</th>\n",
       "      <th>CNV_NOTCH2</th>\n",
       "      <th>CNV_DNM2</th>\n",
       "      <th>CNV_TNFRSF9</th>\n",
       "      <th>CNV_FGF9</th>\n",
       "      <th>CNV_ECT2L</th>\n",
       "      <th>CNV_CDKN1A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GI-19-025</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-17-537</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-19-002</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-16-575</th>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-14-974</th>\n",
       "      <td>-0.914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 648 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           CNV_HLA-DPB1  CNV_GPC3  CNV_SOX2  CNV_XRCC3  CNV_PAK1  CNV_ACVR1B   \n",
       "GI-19-025         0.000       0.0     0.000      0.000     0.000       0.000  \\\n",
       "GI-17-537         0.000       0.0     0.000      0.000     0.000       0.000   \n",
       "GI-19-002         0.000       0.0     0.000      0.000     0.000       0.000   \n",
       "GI-16-575        -0.217       0.0     0.000     -0.007     0.121      -0.174   \n",
       "GI-14-974        -0.914       0.0     0.075      0.004     0.000       0.000   \n",
       "\n",
       "           CNV_DDR2  CNV_FANCL  CNV_CDKN1B  CNV_DYNC2H1  ...  CNV_NBN   \n",
       "GI-19-025     0.000      0.000       0.000        0.000  ...    0.000  \\\n",
       "GI-17-537     0.249      0.000       0.000        0.000  ...    0.000   \n",
       "GI-19-002     0.198     -0.285       0.000        0.000  ...    0.000   \n",
       "GI-16-575     0.000     -0.036      -0.174        0.121  ...   -0.066   \n",
       "GI-14-974    -0.004      0.000       0.000        0.035  ...    0.019   \n",
       "\n",
       "           CNV_HLA-DQB2  CNV_BRCA2  CNV_RAD51C  CNV_NOTCH2  CNV_DNM2   \n",
       "GI-19-025         0.000      0.000       0.000       0.000     0.000  \\\n",
       "GI-17-537         0.000      0.000       0.000       0.000     0.000   \n",
       "GI-19-002         0.000      0.000       0.000       0.198     0.000   \n",
       "GI-16-575        -0.217      0.040      -0.092       0.000     0.208   \n",
       "GI-14-974         0.000      0.108      -0.056      -0.004     0.071   \n",
       "\n",
       "           CNV_TNFRSF9  CNV_FGF9  CNV_ECT2L  CNV_CDKN1A  \n",
       "GI-19-025        0.000     0.000      0.000       0.000  \n",
       "GI-17-537        0.000     0.000      0.000       0.000  \n",
       "GI-19-002       -0.277     0.000      0.000       0.000  \n",
       "GI-16-575        0.000     0.040     -0.217      -0.217  \n",
       "GI-14-974       -0.004     0.108     -0.019      -0.019  \n",
       "\n",
       "[5 rows x 648 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file += 1\n",
    "filename = files[file]\n",
    "data = pd.read_csv(os.path.join(processed_data_path, filename), index_col=0)\n",
    "print(f\"{filename}.shape\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fc27268-883c-4243-ae28-9cde837a09b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freebayes_data.csv.shape (39, 611)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freebayes_SNV_HES4</th>\n",
       "      <th>freebayes_SNV_PPP1R3E</th>\n",
       "      <th>freebayes_SNV_UBA52</th>\n",
       "      <th>freebayes_SNV_FGF20</th>\n",
       "      <th>freebayes_SNV_RIT1</th>\n",
       "      <th>freebayes_SNV_IL6R</th>\n",
       "      <th>freebayes_SNV_SOX2</th>\n",
       "      <th>freebayes_SNV_SYNE1</th>\n",
       "      <th>freebayes_SNV_DOT1L</th>\n",
       "      <th>freebayes_SNV_RAD51D</th>\n",
       "      <th>...</th>\n",
       "      <th>freebayes_SNV_MAP3K1</th>\n",
       "      <th>freebayes_SNV_MIR650-MIR5571</th>\n",
       "      <th>freebayes_SNV_MAGEB1-NR0B1</th>\n",
       "      <th>freebayes_SNV_TAP2.2</th>\n",
       "      <th>freebayes_SNV_SDHD</th>\n",
       "      <th>freebayes_SNV_PMS2</th>\n",
       "      <th>freebayes_SNV_PTPRT</th>\n",
       "      <th>freebayes_SNV_HEY2</th>\n",
       "      <th>freebayes_SNV_SOX9</th>\n",
       "      <th>freebayes_SNV_RICTOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GI-19-025</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-17-537</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-19-002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-16-575</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-14-974</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 611 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           freebayes_SNV_HES4  freebayes_SNV_PPP1R3E  freebayes_SNV_UBA52   \n",
       "GI-19-025                 0.0                    0.0                  0.0  \\\n",
       "GI-17-537                 0.0                    0.0                  0.0   \n",
       "GI-19-002                 0.0                    0.0                  0.0   \n",
       "GI-16-575                 0.0                    0.0                  0.0   \n",
       "GI-14-974                 0.0                    0.0                  0.0   \n",
       "\n",
       "           freebayes_SNV_FGF20  freebayes_SNV_RIT1  freebayes_SNV_IL6R   \n",
       "GI-19-025                  0.0                 0.0                 0.0  \\\n",
       "GI-17-537                  1.0                 0.0                 0.0   \n",
       "GI-19-002                  0.0                 0.0                 0.0   \n",
       "GI-16-575                  0.0                 0.0                 0.0   \n",
       "GI-14-974                  0.0                 0.0                 0.0   \n",
       "\n",
       "           freebayes_SNV_SOX2  freebayes_SNV_SYNE1  freebayes_SNV_DOT1L   \n",
       "GI-19-025                 0.0                  0.0                  0.0  \\\n",
       "GI-17-537                 0.0                  0.0                  0.0   \n",
       "GI-19-002                 0.0                  0.0                  0.0   \n",
       "GI-16-575                 0.0                  0.0                  0.0   \n",
       "GI-14-974                 0.0                  0.0                  0.0   \n",
       "\n",
       "           freebayes_SNV_RAD51D  ...  freebayes_SNV_MAP3K1   \n",
       "GI-19-025                   0.0  ...                   0.0  \\\n",
       "GI-17-537                   0.0  ...                   0.0   \n",
       "GI-19-002                   0.0  ...                   0.0   \n",
       "GI-16-575                   0.0  ...                   0.0   \n",
       "GI-14-974                   0.0  ...                   0.0   \n",
       "\n",
       "           freebayes_SNV_MIR650-MIR5571  freebayes_SNV_MAGEB1-NR0B1   \n",
       "GI-19-025                           0.0                         0.0  \\\n",
       "GI-17-537                           0.0                         0.0   \n",
       "GI-19-002                           0.0                         0.0   \n",
       "GI-16-575                           0.0                         0.0   \n",
       "GI-14-974                           0.0                         0.0   \n",
       "\n",
       "           freebayes_SNV_TAP2.2  freebayes_SNV_SDHD  freebayes_SNV_PMS2   \n",
       "GI-19-025                   0.0                 0.0                 0.0  \\\n",
       "GI-17-537                   0.0                 0.0                 0.0   \n",
       "GI-19-002                   0.0                 0.0                 0.0   \n",
       "GI-16-575                   0.0                 0.0                 0.0   \n",
       "GI-14-974                   0.0                 0.0                 1.0   \n",
       "\n",
       "           freebayes_SNV_PTPRT  freebayes_SNV_HEY2  freebayes_SNV_SOX9   \n",
       "GI-19-025                  0.0                 0.0                 0.0  \\\n",
       "GI-17-537                  0.0                 0.0                 0.0   \n",
       "GI-19-002                  0.0                 0.0                 0.0   \n",
       "GI-16-575                  0.0                 0.0                 0.0   \n",
       "GI-14-974                  0.0                 0.0                 0.0   \n",
       "\n",
       "           freebayes_SNV_RICTOR  \n",
       "GI-19-025                   0.0  \n",
       "GI-17-537                   1.0  \n",
       "GI-19-002                   0.0  \n",
       "GI-16-575                   0.0  \n",
       "GI-14-974                   0.0  \n",
       "\n",
       "[5 rows x 611 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file += 1\n",
    "filename = files[file]\n",
    "data = pd.read_csv(os.path.join(processed_data_path, filename), index_col=0)\n",
    "print(f\"{filename}.shape\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "053a9d82-3717-40c4-9285-decaa568d083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rna_data.csv.shape (39, 2000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rna_expr_IL6ST</th>\n",
       "      <th>rna_expr_LPP</th>\n",
       "      <th>rna_expr_REG1A</th>\n",
       "      <th>rna_expr_WASF2</th>\n",
       "      <th>rna_expr_UBXN7</th>\n",
       "      <th>rna_expr_NFAT5</th>\n",
       "      <th>rna_expr_MT-ND4</th>\n",
       "      <th>rna_expr_NOTCH2</th>\n",
       "      <th>rna_expr_ARID1A</th>\n",
       "      <th>rna_expr_PLEC</th>\n",
       "      <th>...</th>\n",
       "      <th>rna_expr_NISCH</th>\n",
       "      <th>rna_expr_MFN2</th>\n",
       "      <th>rna_expr_RNF103</th>\n",
       "      <th>rna_expr_KCNK1</th>\n",
       "      <th>rna_expr_VASH1</th>\n",
       "      <th>rna_expr_TBC1D4</th>\n",
       "      <th>rna_expr_EPS15L1</th>\n",
       "      <th>rna_expr_MANBA</th>\n",
       "      <th>rna_expr_ZNF561</th>\n",
       "      <th>rna_expr_CHD8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GI-19-025</th>\n",
       "      <td>241.035075</td>\n",
       "      <td>1021.269300</td>\n",
       "      <td>13.729300</td>\n",
       "      <td>147.2934</td>\n",
       "      <td>103.106320</td>\n",
       "      <td>94.383909</td>\n",
       "      <td>14.84040</td>\n",
       "      <td>304.516089</td>\n",
       "      <td>189.958217</td>\n",
       "      <td>324.729539</td>\n",
       "      <td>...</td>\n",
       "      <td>225.176181</td>\n",
       "      <td>39.375050</td>\n",
       "      <td>95.598693</td>\n",
       "      <td>38.908040</td>\n",
       "      <td>64.202540</td>\n",
       "      <td>63.034703</td>\n",
       "      <td>50.252791</td>\n",
       "      <td>32.985904</td>\n",
       "      <td>90.178841</td>\n",
       "      <td>61.951973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-17-537</th>\n",
       "      <td>215.826456</td>\n",
       "      <td>478.585126</td>\n",
       "      <td>8.704680</td>\n",
       "      <td>132.7357</td>\n",
       "      <td>86.787428</td>\n",
       "      <td>78.155881</td>\n",
       "      <td>3.32408</td>\n",
       "      <td>124.369304</td>\n",
       "      <td>154.196223</td>\n",
       "      <td>479.120290</td>\n",
       "      <td>...</td>\n",
       "      <td>133.896703</td>\n",
       "      <td>35.331787</td>\n",
       "      <td>62.410831</td>\n",
       "      <td>45.372260</td>\n",
       "      <td>18.359144</td>\n",
       "      <td>19.237960</td>\n",
       "      <td>29.319210</td>\n",
       "      <td>46.584303</td>\n",
       "      <td>38.170383</td>\n",
       "      <td>68.306816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-19-002</th>\n",
       "      <td>305.901310</td>\n",
       "      <td>506.337486</td>\n",
       "      <td>281.889123</td>\n",
       "      <td>161.4266</td>\n",
       "      <td>114.331597</td>\n",
       "      <td>95.955602</td>\n",
       "      <td>12.47790</td>\n",
       "      <td>183.312681</td>\n",
       "      <td>221.292130</td>\n",
       "      <td>205.174571</td>\n",
       "      <td>...</td>\n",
       "      <td>165.992257</td>\n",
       "      <td>21.930622</td>\n",
       "      <td>194.303080</td>\n",
       "      <td>76.009044</td>\n",
       "      <td>42.496390</td>\n",
       "      <td>79.657814</td>\n",
       "      <td>49.634208</td>\n",
       "      <td>41.942784</td>\n",
       "      <td>93.740426</td>\n",
       "      <td>67.387638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-16-575</th>\n",
       "      <td>232.227397</td>\n",
       "      <td>423.957525</td>\n",
       "      <td>178.389507</td>\n",
       "      <td>172.6827</td>\n",
       "      <td>104.493370</td>\n",
       "      <td>106.009576</td>\n",
       "      <td>18.69510</td>\n",
       "      <td>302.722650</td>\n",
       "      <td>233.581237</td>\n",
       "      <td>282.710050</td>\n",
       "      <td>...</td>\n",
       "      <td>165.535513</td>\n",
       "      <td>36.941221</td>\n",
       "      <td>193.460040</td>\n",
       "      <td>61.266900</td>\n",
       "      <td>56.176480</td>\n",
       "      <td>106.721880</td>\n",
       "      <td>49.656624</td>\n",
       "      <td>43.322795</td>\n",
       "      <td>92.509717</td>\n",
       "      <td>62.555034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI-14-974</th>\n",
       "      <td>263.865278</td>\n",
       "      <td>1120.498530</td>\n",
       "      <td>2177.204700</td>\n",
       "      <td>142.0982</td>\n",
       "      <td>95.410231</td>\n",
       "      <td>87.508449</td>\n",
       "      <td>15.89970</td>\n",
       "      <td>192.934799</td>\n",
       "      <td>260.016111</td>\n",
       "      <td>180.536180</td>\n",
       "      <td>...</td>\n",
       "      <td>137.285121</td>\n",
       "      <td>51.824260</td>\n",
       "      <td>106.984220</td>\n",
       "      <td>16.465899</td>\n",
       "      <td>40.672892</td>\n",
       "      <td>54.942205</td>\n",
       "      <td>64.974170</td>\n",
       "      <td>24.602461</td>\n",
       "      <td>95.077477</td>\n",
       "      <td>75.541299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           rna_expr_IL6ST  rna_expr_LPP  rna_expr_REG1A  rna_expr_WASF2   \n",
       "GI-19-025      241.035075   1021.269300       13.729300        147.2934  \\\n",
       "GI-17-537      215.826456    478.585126        8.704680        132.7357   \n",
       "GI-19-002      305.901310    506.337486      281.889123        161.4266   \n",
       "GI-16-575      232.227397    423.957525      178.389507        172.6827   \n",
       "GI-14-974      263.865278   1120.498530     2177.204700        142.0982   \n",
       "\n",
       "           rna_expr_UBXN7  rna_expr_NFAT5  rna_expr_MT-ND4  rna_expr_NOTCH2   \n",
       "GI-19-025      103.106320       94.383909         14.84040       304.516089  \\\n",
       "GI-17-537       86.787428       78.155881          3.32408       124.369304   \n",
       "GI-19-002      114.331597       95.955602         12.47790       183.312681   \n",
       "GI-16-575      104.493370      106.009576         18.69510       302.722650   \n",
       "GI-14-974       95.410231       87.508449         15.89970       192.934799   \n",
       "\n",
       "           rna_expr_ARID1A  rna_expr_PLEC  ...  rna_expr_NISCH  rna_expr_MFN2   \n",
       "GI-19-025       189.958217     324.729539  ...      225.176181      39.375050  \\\n",
       "GI-17-537       154.196223     479.120290  ...      133.896703      35.331787   \n",
       "GI-19-002       221.292130     205.174571  ...      165.992257      21.930622   \n",
       "GI-16-575       233.581237     282.710050  ...      165.535513      36.941221   \n",
       "GI-14-974       260.016111     180.536180  ...      137.285121      51.824260   \n",
       "\n",
       "           rna_expr_RNF103  rna_expr_KCNK1  rna_expr_VASH1  rna_expr_TBC1D4   \n",
       "GI-19-025        95.598693       38.908040       64.202540        63.034703  \\\n",
       "GI-17-537        62.410831       45.372260       18.359144        19.237960   \n",
       "GI-19-002       194.303080       76.009044       42.496390        79.657814   \n",
       "GI-16-575       193.460040       61.266900       56.176480       106.721880   \n",
       "GI-14-974       106.984220       16.465899       40.672892        54.942205   \n",
       "\n",
       "           rna_expr_EPS15L1  rna_expr_MANBA  rna_expr_ZNF561  rna_expr_CHD8  \n",
       "GI-19-025         50.252791       32.985904        90.178841      61.951973  \n",
       "GI-17-537         29.319210       46.584303        38.170383      68.306816  \n",
       "GI-19-002         49.634208       41.942784        93.740426      67.387638  \n",
       "GI-16-575         49.656624       43.322795        92.509717      62.555034  \n",
       "GI-14-974         64.974170       24.602461        95.077477      75.541299  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file += 1\n",
    "filename = files[file]\n",
    "data = pd.read_csv(os.path.join(processed_data_path, filename), index_col=0)\n",
    "print(f\"{filename}.shape\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b13f9a31-7086-482a-88b6-1d7d34708c43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:44.123341892Z",
     "start_time": "2023-05-23T15:01:28.472433387Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_study = False\n",
    "if new_study:\n",
    "    shutil.rmtree(\"tensorboard/\", ignore_errors= True)\n",
    "    date = time.strftime('%Y%m%d%H')\n",
    "    optimization_study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed = 42, multivariate = True, n_startup_trials = 1000))\n",
    "    n_trials = 1\n",
    "    for file in os.listdir(settings.optimization_path):\n",
    "        try:\n",
    "            os.remove(os.path.join(settings.optimization_path, file))\n",
    "        except IsADirectoryError:\n",
    "            shutil.rmtree(os.path.join(settings.optimization_path, file), ignore_errors= True)\n",
    "else:\n",
    "    date = \"2024050715\"\n",
    "    with open(os.path.join(settings.optimization_path, f'optimization_optuna_{date}.pkl'), 'rb') as file:\n",
    "        optimization_study = dill.load(file)\n",
    "    n_trials = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1016923-cb4c-4a4f-a4b5-398eb4ed0aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a73bfcd021415cbd076925b14d98a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d047ceb9aa3b42bd9c4002441d3eeb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_8be95940-ca4a-4df2-a29e-9aa924a42c61.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_8be95940-ca4a-4df2-a29e-9aa924a42c61.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tensorboard/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926236dc59704ea4a44d634187ad6b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b024b153-c0ea-458a-817d-532fb1630cfe.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b024b153-c0ea-458a-817d-532fb1630cfe.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b43c64dde34bc391a933cb1a6cb6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_69fba3d0-aa68-4625-85ff-f805af180fdc.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_69fba3d0-aa68-4625-85ff-f805af180fdc.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cacbb8b04db42fabdb223798baabbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_361e10cf-7d5f-4bec-9f67-7e7c6f513d51.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_361e10cf-7d5f-4bec-9f67-7e7c6f513d51.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dd9f5316ea44748f3b5e4270b3befd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_cc8fe675-ab69-4475-a9ed-927aa9f58ad5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_cc8fe675-ab69-4475-a9ed-927aa9f58ad5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f23e35e4a5a47d4bc2aac79b257eae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_38428d70-e361-4175-9375-9128b6d040c7.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_38428d70-e361-4175-9375-9128b6d040c7.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dca7d450390470eba21856ba673e381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7fdb2f92-e7f9-4b09-821a-1024c79eaeae.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7fdb2f92-e7f9-4b09-821a-1024c79eaeae.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c10e34e92e24030bb56c3c99e76348a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_8a35e6b9-638e-43ba-ac89-383207f96460.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_8a35e6b9-638e-43ba-ac89-383207f96460.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5169759a1d6e41a3982018ffdc6b37df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1646d0ed-b635-4e80-8f64-8d3beb9baeee.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1646d0ed-b635-4e80-8f64-8d3beb9baeee.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae74ef3cb84349f89f793a4bebf86a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_0dd81aa7-87f1-41b3-a815-f3911f7404bf.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_0dd81aa7-87f1-41b3-a815-f3911f7404bf.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89647568ee7b42fcaec6a4e941900814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_70fa52d1-dcc4-4e35-a093-239b867dcdbe.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_70fa52d1-dcc4-4e35-a093-239b867dcdbe.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7804e828f8384637b9a10430da8e3003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7b8a14ce-bed6-4504-bb20-e9a88a2f83ae.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7b8a14ce-bed6-4504-bb20-e9a88a2f83ae.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fd69109912464298a584161820d952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7c139595-14b6-44f0-9141-90ca26d25edb.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7c139595-14b6-44f0-9141-90ca26d25edb.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903e1448c4cb4ea3840d13348ddf666a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_85705350-c33c-485b-aa08-0b8fe9a2bd74.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_85705350-c33c-485b-aa08-0b8fe9a2bd74.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8832216aa0184a9db1004e9dbb638fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_01d7ea05-1f46-4100-9893-20c74d6b0446.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_01d7ea05-1f46-4100-9893-20c74d6b0446.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9132395a21bd443d831e2b1d2a188040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_180c2d9a-f6c1-4e20-b7d6-7d140c0a3217.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_180c2d9a-f6c1-4e20-b7d6-7d140c0a3217.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c502047aa04659bd20bb6097a34d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_8b41d943-da48-4337-8a64-e6e5de73321f.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_8b41d943-da48-4337-8a64-e6e5de73321f.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e1ceb13b694801bc8ebff0445cd254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_36689944-fdb9-4283-8ab5-7fffa7a15f6d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_36689944-fdb9-4283-8ab5-7fffa7a15f6d.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c97054a7174f798174cda48c95b1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2fa9d2ad-914a-4d4a-ad9f-61427394f586.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2fa9d2ad-914a-4d4a-ad9f-61427394f586.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3440b6c889453487217d5938b40a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6197dd34-b0c2-4fa6-b8ff-247f13a15c3b.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6197dd34-b0c2-4fa6-b8ff-247f13a15c3b.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee84d35dba449f49f2397dcfb73b5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6a47babd-331c-453d-915a-c2b5d8dc1ece.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6a47babd-331c-453d-915a-c2b5d8dc1ece.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851d4a64f804402d9a30be747765c839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d700d084-4b7c-4526-8eab-2e069c31e58c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d700d084-4b7c-4526-8eab-2e069c31e58c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead1413093794f89b3d28fe595fb102b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_410c4d92-584a-4fb6-9d3d-5ee7ffb3281f.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_410c4d92-584a-4fb6-9d3d-5ee7ffb3281f.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474ece34ba064da4b3d70a8018c9b571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_464f1bcd-5ee7-49b4-939f-745309b984d5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_464f1bcd-5ee7-49b4-939f-745309b984d5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0601b80ed941369945fc17dbe7c531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d8c5429f-d3a3-49a6-afa8-7c545a4dc3d7.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d8c5429f-d3a3-49a6-afa8-7c545a4dc3d7.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 187 K \n",
      "13 | decoder_6 | FCN        | 192 K \n",
      "------------------------------------------\n",
      "10.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 M    Total params\n",
      "43.170    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2024-05-07 15:24:21,105]\u001b[0m Trial 1 finished with value: 0.013047620207071304 and parameters: {'num_layers': 1, 'latent_space': 128, 'divisor_units': 4, 'n_clusters': 4, 'n_epochs': 20, 'lambda_coeff': 0.8123245085588685}. Best is trial 1 with value: 0.013047620207071304.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c7bcaf7e1a476fb6fa4a85bdc7ba46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c676d978-192c-4593-8256-dc7de08aafe3.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c676d978-192c-4593-8256-dc7de08aafe3.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tensorboard/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e6434358ad4e9bb6ef118b6353ae47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2f5aa3db-f051-4494-b7f9-be7222fd3c99.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2f5aa3db-f051-4494-b7f9-be7222fd3c99.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d6fccda1a64c28890f86b545b3dcdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_94336f3f-457c-43b6-ac62-a1a6dada0d31.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_94336f3f-457c-43b6-ac62-a1a6dada0d31.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c28d0a90174fa0a2478d9336923d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_3b0f641b-33d4-4e7b-936b-045b7beec100.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_3b0f641b-33d4-4e7b-936b-045b7beec100.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bd6428e00d4de68c1a88770dbf4f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_bcb0523d-7044-4b47-a73b-2f174718d891.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_bcb0523d-7044-4b47-a73b-2f174718d891.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf99a6289630443cac572d17180282d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_31839568-6b9a-4f3d-9879-b5b7841a4396.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_31839568-6b9a-4f3d-9879-b5b7841a4396.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e6371e125747058bad024a72e52903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_e29b1570-dcac-4b1b-844c-f859a11f08af.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_e29b1570-dcac-4b1b-844c-f859a11f08af.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ece6b9fba724cd59fe8b9261cc5e565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_cebb6c06-0022-4f2e-a1c2-b109ef338741.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_cebb6c06-0022-4f2e-a1c2-b109ef338741.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a2570ea163433b830a6ef363f1d4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_4f7f62e6-293c-4694-aacb-7fee9848bfab.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_4f7f62e6-293c-4694-aacb-7fee9848bfab.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2ae18797b24c13b7b4e979c768e726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d95523d9-a49d-492c-9362-8a6e0ca64c59.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d95523d9-a49d-492c-9362-8a6e0ca64c59.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076367e762894a0fb7c542962ec98a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_91bb6f9c-d260-4333-9a0b-8975a9d337a0.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_91bb6f9c-d260-4333-9a0b-8975a9d337a0.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2380b10b744cdf8ad2a9ad179b65d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6269b7a5-7c7f-4ffc-857e-4889168181ea.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6269b7a5-7c7f-4ffc-857e-4889168181ea.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c319fcc6ebab4014a30616d5461b7416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_0f4f5770-faf9-4051-a067-4b6cae222fc5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_0f4f5770-faf9-4051-a067-4b6cae222fc5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2ec0622ed6492d9c5a1ebe8e968a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b9f16d08-bd9d-477a-8306-c45a3d1c61c9.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b9f16d08-bd9d-477a-8306-c45a3d1c61c9.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0126bbd4ade3468cbae3a9cf9b77cd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_5e1e4a6a-e942-4200-9116-b672342fbedf.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_5e1e4a6a-e942-4200-9116-b672342fbedf.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a85eed8eeda440aa89b42357fee332c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6fa04c13-f69e-4f87-897e-d275289f6edd.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6fa04c13-f69e-4f87-897e-d275289f6edd.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5199d4aca6f4f07b1c7910a0b116450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b483b65c-74b0-4182-894a-d5e6aa917a97.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b483b65c-74b0-4182-894a-d5e6aa917a97.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3a774d21664d4f9acaee4b7146f90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c3a542b9-468c-43e1-b586-42d9cbb29a8c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c3a542b9-468c-43e1-b586-42d9cbb29a8c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490c8d8bab5948a7830fb5b432c2313c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d754e838-b59e-4b07-9a66-155bd53ac404.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d754e838-b59e-4b07-9a66-155bd53ac404.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4a7631f2bf49e1a25c8f2969c9b5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_cf80a4c0-e1c2-4376-9fdd-596b84e44e75.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_cf80a4c0-e1c2-4376-9fdd-596b84e44e75.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b86156fbb6c46a7a33eb9099ab1a47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1bf178b3-33ee-41a8-84ab-319e62e08e30.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1bf178b3-33ee-41a8-84ab-319e62e08e30.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40adaefd636b4deba808efaafb9f74bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_25243812-0053-4f11-8232-0a163aff2794.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_25243812-0053-4f11-8232-0a163aff2794.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b88a4981f941bc8b37276c6b064850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_85a992f0-b74f-40b6-9da4-05c58c7aff26.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_85a992f0-b74f-40b6-9da4-05c58c7aff26.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5947f1ac196a4126b2d6ed5012283499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a57b11fe-757c-418a-8f76-fc3da01dc3c3.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a57b11fe-757c-418a-8f76-fc3da01dc3c3.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e2ec592b7f45718b906b0e01d0f55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_029af04c-429a-4ecf-bd2a-a9e8017b7231.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_029af04c-429a-4ecf-bd2a-a9e8017b7231.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2024-05-07 15:32:24,020]\u001b[0m Trial 2 finished with value: 0.0014310326334089042 and parameters: {'num_layers': 2, 'latent_space': 32, 'divisor_units': 2, 'n_clusters': 2, 'n_epochs': 40, 'lambda_coeff': 0.03752055855124281}. Best is trial 1 with value: 0.013047620207071304.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299bcfa828ac4dbc8ac914a5f7123621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_f3cd4566-ba6c-402c-a653-b196aea5bb75.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_f3cd4566-ba6c-402c-a653-b196aea5bb75.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tensorboard/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a04d3e74574458a94d68cef8c6ed775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_5c142b3a-678d-4b04-9f44-12c0eb04231e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_5c142b3a-678d-4b04-9f44-12c0eb04231e.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab2ada3a518480c89db82c02af78fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1253f476-67cd-4df1-b04d-450792c4720b.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1253f476-67cd-4df1-b04d-450792c4720b.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac71a95855c4b6c89348ad8c92b4ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_54d09d06-8b9e-4773-a6ec-176055e6b84a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_54d09d06-8b9e-4773-a6ec-176055e6b84a.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9220d000a5104896920f10def26f24fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b6952bc0-32cb-4211-bcce-3a508f28400a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b6952bc0-32cb-4211-bcce-3a508f28400a.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad9235e1ef341ccb8db3d9a456fda49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_9bad9644-da3d-42d7-ad63-b3cb4743b7c0.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_9bad9644-da3d-42d7-ad63-b3cb4743b7c0.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04f3a0c0fab47ec94af811c27ce1b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b7afb3ba-c0f7-4bc4-a389-96b45995e265.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b7afb3ba-c0f7-4bc4-a389-96b45995e265.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a0910de64c4074afcc015659322cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_16858bbf-063a-4480-9be3-e1b0224f2ba4.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_16858bbf-063a-4480-9be3-e1b0224f2ba4.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c753806b714291ba7bba6e2a932f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.001584893192461114\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1d853661-c0f8-43ef-a13d-8b434b1985c4.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1d853661-c0f8-43ef-a13d-8b434b1985c4.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4a02f051bf478c9ce8d53f1f71889a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_52c46ce8-82ec-43ce-bc99-8ab5ba9b3133.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_52c46ce8-82ec-43ce-bc99-8ab5ba9b3133.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6246a53730dd4008a33e72120c68bcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_83562f64-927f-4e86-b7ae-365eb2d5d1a2.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_83562f64-927f-4e86-b7ae-365eb2d5d1a2.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472d7a06b6834660ad3e001eaf6e2eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1e316ed0-41f8-485c-af26-a41b8484975c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1e316ed0-41f8-485c-af26-a41b8484975c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b7aa4016a9458789a2365e82f99b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_f22e77df-ba28-40d4-8bbf-196a26010bbf.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_f22e77df-ba28-40d4-8bbf-196a26010bbf.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4a212c116345709e0934d9147bb2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c1599615-4ac7-4549-9f6a-f918fab7649a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c1599615-4ac7-4549-9f6a-f918fab7649a.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d538d1584a4d1e9dcc5400e05f5ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_54d8ab96-7845-408e-b75f-75afdca2afa8.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_54d8ab96-7845-408e-b75f-75afdca2afa8.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd649244246f46cb9f189afa9e00ab8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_fcca720a-0f97-4857-ab3d-24fb820f06d8.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_fcca720a-0f97-4857-ab3d-24fb820f06d8.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc07e7324dcf4b358c1cf5c06aa34676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.003311311214825908\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a13ae882-c939-434e-bcb8-080f4f62394e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a13ae882-c939-434e-bcb8-080f4f62394e.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fbfb2cf6d7460bbb24a84b72831395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1d85100c-be26-4136-be0e-b18b039cd72f.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1d85100c-be26-4136-be0e-b18b039cd72f.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9327b8ad2c304f1ea3e63bcfe6929061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_cf4e7be9-28f3-4895-b951-58d3fdee4ab7.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_cf4e7be9-28f3-4895-b951-58d3fdee4ab7.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab7f8df7f534be4916387fb49e8cf2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.003311311214825908\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2f8f5d4b-2ffb-48b2-825a-a68549e68bf7.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2f8f5d4b-2ffb-48b2-825a-a68549e68bf7.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fce5b3f46024bc6b28ba1ab277cc64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_ac5552f8-4cc7-4a5f-af74-a31829ff9184.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_ac5552f8-4cc7-4a5f-af74-a31829ff9184.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2074f5afd342cc83858eb946a44e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7795d42c-a5e6-4123-ac84-39bcab993f76.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7795d42c-a5e6-4123-ac84-39bcab993f76.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab48e4e1fbb84c5487df16a77b43854d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_afe20851-3cce-4388-8489-9ccc299172a9.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_afe20851-3cce-4388-8489-9ccc299172a9.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6c82859b13437091afd9d4680295f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6e4b240e-8335-4669-b83b-b79007f436fc.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6e4b240e-8335-4669-b83b-b79007f436fc.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c88a95e632c41fb96eec717359b7d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_698c634b-baa0-4bcf-9049-2393b09c584e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_698c634b-baa0-4bcf-9049-2393b09c584e.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 109 K \n",
      "1  | decoder_0 | FCN        | 972 K \n",
      "2  | encoder_1 | FCN        | 319 K \n",
      "3  | decoder_1 | FCN        | 1.7 M \n",
      "4  | encoder_2 | FCN        | 168 K \n",
      "5  | decoder_2 | FCN        | 1.2 M \n",
      "6  | encoder_3 | FCN        | 105 K \n",
      "7  | decoder_3 | FCN        | 950 K \n",
      "8  | encoder_4 | FCN        | 93.3 K\n",
      "9  | decoder_4 | FCN        | 896 K \n",
      "10 | encoder_5 | FCN        | 1.0 M \n",
      "11 | decoder_5 | FCN        | 2.9 M \n",
      "12 | encoder_6 | Sequential | 93.9 K\n",
      "13 | decoder_6 | FCN        | 98.2 K\n",
      "------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.419    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2024-05-07 15:38:31,404]\u001b[0m Trial 3 finished with value: 0.005508788228034973 and parameters: {'num_layers': 1, 'latent_space': 64, 'divisor_units': 4, 'n_clusters': 2, 'n_epochs': 40, 'lambda_coeff': 0.01256277350380703}. Best is trial 1 with value: 0.013047620207071304.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfff122d8f82414bae1f1ac60a7eee37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_9a4c2bef-b433-4cca-90f2-36affee70b8f.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_9a4c2bef-b433-4cca-90f2-36affee70b8f.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tensorboard/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a9083da14946e8bd45bf1b231c2905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c540dd9b-a503-4288-9af3-faf293f3be98.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c540dd9b-a503-4288-9af3-faf293f3be98.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bea3d94c1814320ac58907a12a7d138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_59a25f52-01f3-4451-a5f4-9a93769fee43.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_59a25f52-01f3-4451-a5f4-9a93769fee43.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef28bf32345492b80c1dbaff30b8ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_20a37b74-234f-4819-98c1-dd5a7005ea1b.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_20a37b74-234f-4819-98c1-dd5a7005ea1b.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7ec99636034137897b21b1c6b7c48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_39b47092-a2d9-4e25-a8fc-01f47dc6ca80.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_39b47092-a2d9-4e25-a8fc-01f47dc6ca80.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b2a6aed81047acadfc5da98180805f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_5e89155d-9b46-402a-8cad-9fbdefbad5d8.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_5e89155d-9b46-402a-8cad-9fbdefbad5d8.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52309ab8dfe43d19df7693920ac4721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c6705578-8766-43fa-b177-f91b34f6845d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c6705578-8766-43fa-b177-f91b34f6845d.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86e15a9259e4491a3de28821fbef0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_57730b61-f950-4f7d-8bf4-065348c8be6a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_57730b61-f950-4f7d-8bf4-065348c8be6a.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43c75bbd6804694bd2c640393e3aa60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a0ab96df-7ecd-49df-893d-ae1855326437.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a0ab96df-7ecd-49df-893d-ae1855326437.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0eb5cd692b450dac7e310bc80d7eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_09fdfbf4-1fd6-4b8e-a0ac-86055976f418.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_09fdfbf4-1fd6-4b8e-a0ac-86055976f418.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc674533dd94fc7bf629f22159cca8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_14fe3397-17da-47cb-9348-7ed72cacd054.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_14fe3397-17da-47cb-9348-7ed72cacd054.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad347f2bcdd4ad2998840ae35f1c916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_47126a70-3164-4e75-9ae1-34d1f85ee5cf.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_47126a70-3164-4e75-9ae1-34d1f85ee5cf.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd4369ecd184627bde7a14684e794f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_fd9b84b1-d334-4f5b-9c39-27c8efe0f59b.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_fd9b84b1-d334-4f5b-9c39-27c8efe0f59b.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddd3d307de14a6c800547308663ef7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2afad39c-ae96-4643-99c1-1e14d634fb9b.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2afad39c-ae96-4643-99c1-1e14d634fb9b.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2801506ac8d7402facf6a93fe64db116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_4539f458-6b70-46a3-b418-97d328e587a5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_4539f458-6b70-46a3-b418-97d328e587a5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9388364018424e8921b3aaee3ff816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_e84ca85e-88e8-4040-b2df-1b2fb048a82d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_e84ca85e-88e8-4040-b2df-1b2fb048a82d.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f8b194a8b04486a1177387824637b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_932ec789-a2d7-4a39-be68-48333eb737ef.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_932ec789-a2d7-4a39-be68-48333eb737ef.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8570b95cdfe48809b5918414ed729ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_571e9ff4-d444-4493-85c6-1924f445ee60.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_571e9ff4-d444-4493-85c6-1924f445ee60.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5fbb58562c475ab3e77c4d11104248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_aa0b483c-f9ec-458b-b845-410d90fd057c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_aa0b483c-f9ec-458b-b845-410d90fd057c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1e2bd5f1a84b66bf53c2d1b441b24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_44a8c9b1-cf0a-43db-86b5-b007223abab9.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_44a8c9b1-cf0a-43db-86b5-b007223abab9.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1af585d24d94d24b311a86505af980e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_66f8bfcc-938c-4ab2-bce3-fe02e567a743.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_66f8bfcc-938c-4ab2-bce3-fe02e567a743.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901f164ae72545fe95fdac5f8c0eeff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_ce925563-8867-424b-bd18-10c12806c067.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_ce925563-8867-424b-bd18-10c12806c067.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196d816974d4453da37c7913ecad227e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1aa65aae-740e-4d00-8c41-e17c9c33f943.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1aa65aae-740e-4d00-8c41-e17c9c33f943.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1804cfd02f3143cb880a38dd334e1863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_e79aab4f-2d8b-4096-96a8-c4eb72aaa322.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_e79aab4f-2d8b-4096-96a8-c4eb72aaa322.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a608f869e9974afbbb81043359dea93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_f51a5889-072a-4b78-bd0f-4ee6ae393c9c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_f51a5889-072a-4b78-bd0f-4ee6ae393c9c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2024-05-07 15:55:30,713]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'num_layers': 1, 'latent_space': 128, 'divisor_units': 2, 'n_clusters': 3, 'n_epochs': 60, 'lambda_coeff': 0.001378323745500718}. Best is trial 1 with value: 0.013047620207071304.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2106de15a8456a80717c21fa0afc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2a658f91-abde-4ee1-b38c-ecb64822586f.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2a658f91-abde-4ee1-b38c-ecb64822586f.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tensorboard/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be4423c5ee645378eb041576d13578d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_fb7704e7-98c8-40ff-97b7-c3276331516c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_fb7704e7-98c8-40ff-97b7-c3276331516c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd31950d0a8f4a848c9fa17aacae211b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c50ca5b9-c7d5-4837-b83a-9aa60301cc47.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c50ca5b9-c7d5-4837-b83a-9aa60301cc47.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b876958b9f284fd590c076c5e92ce811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c548b415-5219-4a23-9a81-58820d81e804.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c548b415-5219-4a23-9a81-58820d81e804.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d742a55ae845278872e3b0650675bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1e7b4d5b-e891-4796-b08f-f735cee5e33f.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1e7b4d5b-e891-4796-b08f-f735cee5e33f.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23d67834bab4f48bbd98b39adf9b826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a5df0d82-6023-4c69-9e6d-a482d1183f76.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a5df0d82-6023-4c69-9e6d-a482d1183f76.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f608ea5b5ca74dd29c86f5da972a8a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_9523e4ec-9a71-439f-bed4-0daba885741c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_9523e4ec-9a71-439f-bed4-0daba885741c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262c709c3f084bea8a42cd99f7801e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2627cffc-8018-400f-ba2c-d3e4da24a75c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2627cffc-8018-400f-ba2c-d3e4da24a75c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190686e85025481f9a09c111cd8a9788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_f46f8e66-9668-44b0-946a-a84b1d58a47b.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_f46f8e66-9668-44b0-946a-a84b1d58a47b.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa8bdcc824a425e99d99fb6c19eac11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a36dea29-c09b-4c8f-be6e-4440565ff3e8.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a36dea29-c09b-4c8f-be6e-4440565ff3e8.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782dc783cdb5465898037b3eca274864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a48602fc-30a2-47d8-b740-30fd383b23aa.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a48602fc-30a2-47d8-b740-30fd383b23aa.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c84af543494a4db8e0d17efd92a0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_67e80f68-589c-41c8-a125-e9bbcd9dd262.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_67e80f68-589c-41c8-a125-e9bbcd9dd262.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef4d00c6d3648a29004c83f7189717c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_52879c88-f1ea-49e4-a0e2-212c284ec0f2.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_52879c88-f1ea-49e4-a0e2-212c284ec0f2.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3897b4a7d194d67b607082c7ddf2fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_fc930e05-2632-4f5b-bfde-729d8a75bb98.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_fc930e05-2632-4f5b-bfde-729d8a75bb98.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9858d2c72b7d4b2181d3d1ec1c373377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_612fd8ae-ba96-4ed4-ba9b-0e510f43d27c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_612fd8ae-ba96-4ed4-ba9b-0e510f43d27c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008e5daf39d9464494a72a54be02c422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2e2ab8f4-8bd8-40ed-85be-c74f00fccaa5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2e2ab8f4-8bd8-40ed-85be-c74f00fccaa5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9760c5725c4421a65d5ce64961e153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d47eaf25-8fee-4964-a6ff-c3f7743fc8e6.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d47eaf25-8fee-4964-a6ff-c3f7743fc8e6.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd83c25eebd4fe19de972e062266a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a2cd1b35-cc3a-429f-96ee-5af0246e8c25.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a2cd1b35-cc3a-429f-96ee-5af0246e8c25.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2924c8a5cf4bdc8da9623320e12808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_07e0bcf2-e3d3-430a-bad7-7c06e3be192d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_07e0bcf2-e3d3-430a-bad7-7c06e3be192d.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c5c920c3d04030a6ae6adfa9a48776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a530737d-6d0a-4529-a5d1-2c52e63995a1.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a530737d-6d0a-4529-a5d1-2c52e63995a1.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8012981a39664d08976bc5e8bcb2dc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_62ca6aba-222f-45c2-a2ba-aca69a403be5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_62ca6aba-222f-45c2-a2ba-aca69a403be5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9397e89dbde14c7a97991288b8ffc777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_3215455e-1611-48bd-9a4a-272c0db1f31c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_3215455e-1611-48bd-9a4a-272c0db1f31c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe51c6e0300418bb8692664584c6d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_14d27e77-b074-41d6-af50-4fdb4ce29548.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_14d27e77-b074-41d6-af50-4fdb4ce29548.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe5218091d84a1c9f11815a51f6bff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_ba7cc30c-dd56-40dd-8a38-c4004e1c30a0.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_ba7cc30c-dd56-40dd-8a38-c4004e1c30a0.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8b6ac8718e4e8aa4ce0c28185f4c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_fa5b7eed-4345-4521-a185-6309cf5984e2.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_fa5b7eed-4345-4521-a185-6309cf5984e2.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 275 K \n",
      "1  | decoder_0 | FCN        | 706 K \n",
      "2  | encoder_1 | FCN        | 800 K \n",
      "3  | decoder_1 | FCN        | 1.5 M \n",
      "4  | encoder_2 | FCN        | 422 K \n",
      "5  | decoder_2 | FCN        | 939 K \n",
      "6  | encoder_3 | FCN        | 263 K \n",
      "7  | decoder_3 | FCN        | 686 K \n",
      "8  | encoder_4 | FCN        | 234 K \n",
      "9  | decoder_4 | FCN        | 635 K \n",
      "10 | encoder_5 | FCN        | 2.5 M \n",
      "11 | decoder_5 | FCN        | 3.5 M \n",
      "12 | encoder_6 | Sequential | 46.9 K\n",
      "13 | decoder_6 | FCN        | 51.3 K\n",
      "------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.026    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2024-05-07 16:13:06,025]\u001b[0m Trial 5 finished with value: 0.01907716547138989 and parameters: {'num_layers': 2, 'latent_space': 32, 'divisor_units': 2, 'n_clusters': 4, 'n_epochs': 100, 'lambda_coeff': 0.2661901888489056}. Best is trial 5 with value: 0.01907716547138989.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce764c329344ab390bf98d33c5156bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.003311311214825908\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_dc6de14e-3075-461f-a4ed-78c3fda61377.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_dc6de14e-3075-461f-a4ed-78c3fda61377.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tensorboard/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc557d1e6fd47e7b380fe198e1722a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_9f3a6b49-bb46-44cd-a626-46c668362d7a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_9f3a6b49-bb46-44cd-a626-46c668362d7a.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ee3ea256434f1ea9e9b7e4df9fae29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c6602082-6f67-4a9f-8ace-ab1d91bf77c5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c6602082-6f67-4a9f-8ace-ab1d91bf77c5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472d218216134056af2ae3cd1000513f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1d48d363-bb8e-4115-8649-8bbcac960ab5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_1d48d363-bb8e-4115-8649-8bbcac960ab5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be2252fd06c46a4b574494d9a87c40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.012022644346174132\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_0adbcd2a-d998-4710-8d69-796c96dadeaf.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_0adbcd2a-d998-4710-8d69-796c96dadeaf.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d41b401ac3f481e940f118805a21b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.003311311214825908\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_fe176228-61f2-4849-901f-d6c0d5105e06.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_fe176228-61f2-4849-901f-d6c0d5105e06.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0389d0a3c6df439f9eac766fa394a9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.003981071705534969\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_dd8034b9-12c7-4eac-9ee5-8c5b08b425c5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_dd8034b9-12c7-4eac-9ee5-8c5b08b425c5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166ba4b9c5a643d8b141080ade11bd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.05248074602497723\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2ee9b54b-08b7-40a9-aebe-4a877d44b2a3.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_2ee9b54b-08b7-40a9-aebe-4a877d44b2a3.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3830a10bcc8c45a3ae4fd5860923d2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.012022644346174132\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_9ebdc98b-3276-43b8-9fbb-cf5076fce85b.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_9ebdc98b-3276-43b8-9fbb-cf5076fce85b.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99982d60bc64467bbaf205a909ea470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.003311311214825908\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c9e2fe03-99d5-4c98-adf0-8531d3b0824e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c9e2fe03-99d5-4c98-adf0-8531d3b0824e.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6073b65a05e24648af2c5d632b0ba9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.01\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_50d75c7d-0657-49ac-8199-a84f26b0fd33.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_50d75c7d-0657-49ac-8199-a84f26b0fd33.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365fe99f524b4a9daabb4455aa88b9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.002754228703338169\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7ed3b08e-bc37-4baf-a0b3-ac181c26399f.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7ed3b08e-bc37-4baf-a0b3-ac181c26399f.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffa1dffdd664516b56ec0e6cb04efc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.002754228703338169\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_705ec859-c2d3-4af8-b689-015e537d12f5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_705ec859-c2d3-4af8-b689-015e537d12f5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223d761ec20e4645a5172b2575110d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.05248074602497723\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_4f14b934-4efb-4c5f-9905-6fd8591779ba.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_4f14b934-4efb-4c5f-9905-6fd8591779ba.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7537237d1a454c6d87cc0c8975886f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.05248074602497723\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7f6bed33-deee-4684-ac59-b3b373a67df8.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7f6bed33-deee-4684-ac59-b3b373a67df8.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d96716057c4607acdd522f0c871795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.003311311214825908\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_aa6d8545-46b7-4c42-abe3-835ff98d2c57.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_aa6d8545-46b7-4c42-abe3-835ff98d2c57.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a37a35a8034f3792a3b43d1b64d9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.006918309709189364\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_89c4dcf5-f1bb-4e10-b41b-5146f16e4394.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_89c4dcf5-f1bb-4e10-b41b-5146f16e4394.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea397fa0510c45cf9e27785454045e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.003311311214825908\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_87f601c1-921a-40dc-85e7-0a8ee844de59.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_87f601c1-921a-40dc-85e7-0a8ee844de59.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aae20ca9a4a4efd86121dfaccb2bbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.036307805477010104\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7065ab35-900a-47b9-a710-7219ad04d706.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7065ab35-900a-47b9-a710-7219ad04d706.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3f1e386ea440d99b87b3f313c4819a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.003311311214825908\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_98285b70-0a6b-4dcb-a0e3-2f4aa614ea8a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_98285b70-0a6b-4dcb-a0e3-2f4aa614ea8a.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8d1109271f4d6ab27b7e4bf3740936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c256d107-40dc-423c-a9aa-facdbf711479.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c256d107-40dc-423c-a9aa-facdbf711479.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61197141b7bf44cc893536f4407d57af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.01\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_04881e50-00d7-4ea0-91c8-fd6557385b7a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_04881e50-00d7-4ea0-91c8-fd6557385b7a.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20a2f225f7440f4a5c678dc042ac504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_20b2f303-3fe5-4c6f-84f9-fa27b9ecb4e5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_20b2f303-3fe5-4c6f-84f9-fa27b9ecb4e5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8515a9b6394333abc5ca0ea780b003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.01445439770745928\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_204a8318-f072-47ff-b634-424c681272f0.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_204a8318-f072-47ff-b634-424c681272f0.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da997c1f84649419db237d8a9add03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.008317637711026709\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c7731a99-c70d-424d-9fc7-189f2e0b0bc7.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c7731a99-c70d-424d-9fc7-189f2e0b0bc7.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 73.3 K\n",
      "1  | decoder_0 | FCN        | 647 K \n",
      "2  | encoder_1 | FCN        | 213 K \n",
      "3  | decoder_1 | FCN        | 1.1 M \n",
      "4  | encoder_2 | FCN        | 111 K \n",
      "5  | decoder_2 | FCN        | 801 K \n",
      "6  | encoder_3 | FCN        | 70.3 K\n",
      "7  | decoder_3 | FCN        | 633 K \n",
      "8  | encoder_4 | FCN        | 62.0 K\n",
      "9  | decoder_4 | FCN        | 596 K \n",
      "10 | encoder_5 | FCN        | 667 K \n",
      "11 | decoder_5 | FCN        | 2.0 M \n",
      "12 | encoder_6 | Sequential | 31.3 K\n",
      "13 | decoder_6 | FCN        | 34.2 K\n",
      "------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.000    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2024-05-07 16:15:36,650]\u001b[0m Trial 6 finished with value: 0.016818033158779146 and parameters: {'num_layers': 1, 'latent_space': 32, 'divisor_units': 6, 'n_clusters': 3, 'n_epochs': 20, 'lambda_coeff': 0.030586566669785254}. Best is trial 5 with value: 0.01907716547138989.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ece7c4213f4ef39c80332eef440349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_cde56b13-3162-4766-8f87-29a38f3a22e0.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_cde56b13-3162-4766-8f87-29a38f3a22e0.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tensorboard/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef88c02c5b1f4080bc4875b7c5332613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c128f1ff-e843-43a6-8efa-b34126f31a12.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c128f1ff-e843-43a6-8efa-b34126f31a12.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d215eac7cb34f709aa0ec127a2f19c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a548faec-7921-4bb7-a64a-0a71e8854410.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a548faec-7921-4bb7-a64a-0a71e8854410.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caf1e2dcf724c47be38513414b1bddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_051a73bc-d0f8-45fa-8900-65b1fbbc5341.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_051a73bc-d0f8-45fa-8900-65b1fbbc5341.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5619a589ee4c5cbf606cb3ca050ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_52caec0b-1a37-44c9-8474-718a12e4e523.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_52caec0b-1a37-44c9-8474-718a12e4e523.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d540f73ee46b40eeaa7b8e7259705880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_174bf4b8-1235-4ba6-a372-dfa63fbca552.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_174bf4b8-1235-4ba6-a372-dfa63fbca552.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e345fbbb745e4cf09795f3121799b53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a3e955c8-f464-47f2-8dc1-a15b09a5b070.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_a3e955c8-f464-47f2-8dc1-a15b09a5b070.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec61ab535535441fb3c8c3278f80a09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_20069ae0-0ae4-465a-b841-d4ec5f955eaf.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_20069ae0-0ae4-465a-b841-d4ec5f955eaf.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89003597c9544e4783087d53d254002e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_0447783e-6a53-4768-87ae-1f504d2f30a3.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_0447783e-6a53-4768-87ae-1f504d2f30a3.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39570495625f440cb04a1f57e19f78bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_9f9b714f-6444-45c4-a338-a90f544d2bab.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_9f9b714f-6444-45c4-a338-a90f544d2bab.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04043e307f5d457baa94f196bc0da3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_254c8ac3-cf59-443e-99e2-b6e8fbdb068a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_254c8ac3-cf59-443e-99e2-b6e8fbdb068a.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7962b8b81c4dd8b352e944b46357b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c0839fed-5c1c-4c31-b393-77c09068b0fa.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c0839fed-5c1c-4c31-b393-77c09068b0fa.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4860215d9b6d45faa1daf4db99b667bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6d19423e-16d3-45c8-a8c7-69fcf9ab5465.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6d19423e-16d3-45c8-a8c7-69fcf9ab5465.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65675a0257b64b969cf5dcdf4cecdcdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6cd82327-da89-47bc-9679-7fd9e0bd08cc.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_6cd82327-da89-47bc-9679-7fd9e0bd08cc.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd623b65f8a942c38036fd538f3e1ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_36325d00-ba62-441b-8fb6-bbe27cdbd680.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_36325d00-ba62-441b-8fb6-bbe27cdbd680.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c4111989394e52b2b0003b4a2c2336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_8226588b-0af3-415c-9c5f-5f5578906186.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_8226588b-0af3-415c-9c5f-5f5578906186.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80dea1a973d4e6aa27b5e4cbb6ebe85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b1080508-db43-47f9-a326-62edd0e12c83.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b1080508-db43-47f9-a326-62edd0e12c83.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd14c16838fd41cfb8b9fad7c10781d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d76ead6c-4e69-4aa5-916a-ff7aee0719ff.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d76ead6c-4e69-4aa5-916a-ff7aee0719ff.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1a9f66ce9b47d9a3fcfec5392848e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_32ad61e0-402d-495f-8893-b2e0d6f773e2.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_32ad61e0-402d-495f-8893-b2e0d6f773e2.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea46cf8731c044898e5bad35e134383e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_31474f72-f058-4cf4-bcfd-47f12946aa3a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_31474f72-f058-4cf4-bcfd-47f12946aa3a.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c15203bda5a4e00813aa2edf4697b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c916d6fc-dcb0-4b33-ae01-a6aa99ac5609.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_c916d6fc-dcb0-4b33-ae01-a6aa99ac5609.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fae27fba2a40b8811371b898f13412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_0efa10ff-0399-4f81-a321-2bb168f25bd9.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_0efa10ff-0399-4f81-a321-2bb168f25bd9.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b26bb7b0fc145e7837964dc85fd76ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d59c83ed-3fd6-4827-b5cd-8a752d7e5c19.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d59c83ed-3fd6-4827-b5cd-8a752d7e5c19.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50f8b9e1bf345429e5ad8ce99bbcb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_00f3acb9-6964-45fc-9213-75f9943e492f.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_00f3acb9-6964-45fc-9213-75f9943e492f.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a68b7afddc4e86a769608d0d3600f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_ce16d006-4e7a-4ab9-b078-63bf6892f407.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_ce16d006-4e7a-4ab9-b078-63bf6892f407.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 220 K \n",
      "1  | decoder_0 | FCN        | 1.9 M \n",
      "2  | encoder_1 | FCN        | 640 K \n",
      "3  | decoder_1 | FCN        | 3.3 M \n",
      "4  | encoder_2 | FCN        | 337 K \n",
      "5  | decoder_2 | FCN        | 2.4 M \n",
      "6  | encoder_3 | FCN        | 210 K \n",
      "7  | decoder_3 | FCN        | 1.9 M \n",
      "8  | encoder_4 | FCN        | 187 K \n",
      "9  | decoder_4 | FCN        | 1.8 M \n",
      "10 | encoder_5 | FCN        | 2.0 M \n",
      "11 | decoder_5 | FCN        | 5.9 M \n",
      "12 | encoder_6 | Sequential | 375 K \n",
      "13 | decoder_6 | FCN        | 384 K \n",
      "------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.399    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[I 2024-05-07 16:26:40,067]\u001b[0m Trial 7 finished with value: 0.0 and parameters: {'num_layers': 1, 'latent_space': 128, 'divisor_units': 2, 'n_clusters': 3, 'n_epochs': 40, 'lambda_coeff': 0.03632486956676605}. Best is trial 5 with value: 0.01907716547138989.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c50e557798c4c4d90b4b29c554b3a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.025118864315095822\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d22ba548-3503-4b6f-8c88-b9e16305e6b7.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_d22ba548-3503-4b6f-8c88-b9e16305e6b7.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tensorboard/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 75.3 K\n",
      "1  | decoder_0 | FCN        | 91.5 K\n",
      "2  | encoder_1 | FCN        | 218 K \n",
      "3  | decoder_1 | FCN        | 244 K \n",
      "4  | encoder_2 | FCN        | 114 K \n",
      "5  | decoder_2 | FCN        | 134 K \n",
      "6  | encoder_3 | FCN        | 72.3 K\n",
      "7  | decoder_3 | FCN        | 88.2 K\n",
      "8  | encoder_4 | FCN        | 63.7 K\n",
      "9  | decoder_4 | FCN        | 78.8 K\n",
      "10 | encoder_5 | FCN        | 685 K \n",
      "11 | decoder_5 | FCN        | 722 K \n",
      "12 | encoder_6 | Sequential | 5.2 K \n",
      "13 | decoder_6 | FCN        | 5.6 K \n",
      "------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.404    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddd73b1256647e38332f6b52138dbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.025118864315095822\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_235dac36-c542-49f5-8ed3-36f006c9bf68.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_235dac36-c542-49f5-8ed3-36f006c9bf68.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 75.3 K\n",
      "1  | decoder_0 | FCN        | 91.5 K\n",
      "2  | encoder_1 | FCN        | 218 K \n",
      "3  | decoder_1 | FCN        | 244 K \n",
      "4  | encoder_2 | FCN        | 114 K \n",
      "5  | decoder_2 | FCN        | 134 K \n",
      "6  | encoder_3 | FCN        | 72.3 K\n",
      "7  | decoder_3 | FCN        | 88.2 K\n",
      "8  | encoder_4 | FCN        | 63.7 K\n",
      "9  | decoder_4 | FCN        | 78.8 K\n",
      "10 | encoder_5 | FCN        | 685 K \n",
      "11 | decoder_5 | FCN        | 722 K \n",
      "12 | encoder_6 | Sequential | 5.2 K \n",
      "13 | decoder_6 | FCN        | 5.6 K \n",
      "------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.404    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9f926159af4ff689cfb7fe73d174f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.017378008287493765\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7cf4e348-d6f1-46e2-94f5-43bb918625c7.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_7cf4e348-d6f1-46e2-94f5-43bb918625c7.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 75.3 K\n",
      "1  | decoder_0 | FCN        | 91.5 K\n",
      "2  | encoder_1 | FCN        | 218 K \n",
      "3  | decoder_1 | FCN        | 244 K \n",
      "4  | encoder_2 | FCN        | 114 K \n",
      "5  | decoder_2 | FCN        | 134 K \n",
      "6  | encoder_3 | FCN        | 72.3 K\n",
      "7  | decoder_3 | FCN        | 88.2 K\n",
      "8  | encoder_4 | FCN        | 63.7 K\n",
      "9  | decoder_4 | FCN        | 78.8 K\n",
      "10 | encoder_5 | FCN        | 685 K \n",
      "11 | decoder_5 | FCN        | 722 K \n",
      "12 | encoder_6 | Sequential | 5.2 K \n",
      "13 | decoder_6 | FCN        | 5.6 K \n",
      "------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.404    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7246cdb0b36d44fca183d0e1a3505052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.025118864315095822\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_aa3ab162-8a22-4b23-a157-ecd67afd0df4.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_aa3ab162-8a22-4b23-a157-ecd67afd0df4.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 75.3 K\n",
      "1  | decoder_0 | FCN        | 91.5 K\n",
      "2  | encoder_1 | FCN        | 218 K \n",
      "3  | decoder_1 | FCN        | 244 K \n",
      "4  | encoder_2 | FCN        | 114 K \n",
      "5  | decoder_2 | FCN        | 134 K \n",
      "6  | encoder_3 | FCN        | 72.3 K\n",
      "7  | decoder_3 | FCN        | 88.2 K\n",
      "8  | encoder_4 | FCN        | 63.7 K\n",
      "9  | decoder_4 | FCN        | 78.8 K\n",
      "10 | encoder_5 | FCN        | 685 K \n",
      "11 | decoder_5 | FCN        | 722 K \n",
      "12 | encoder_6 | Sequential | 5.2 K \n",
      "13 | decoder_6 | FCN        | 5.6 K \n",
      "------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.404    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8a3d50ee9e481c9417f09c308e5df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.02089296130854041\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_25209d54-0e4f-46c7-9cfb-4367e13e267e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_25209d54-0e4f-46c7-9cfb-4367e13e267e.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 75.3 K\n",
      "1  | decoder_0 | FCN        | 91.5 K\n",
      "2  | encoder_1 | FCN        | 218 K \n",
      "3  | decoder_1 | FCN        | 244 K \n",
      "4  | encoder_2 | FCN        | 114 K \n",
      "5  | decoder_2 | FCN        | 134 K \n",
      "6  | encoder_3 | FCN        | 72.3 K\n",
      "7  | decoder_3 | FCN        | 88.2 K\n",
      "8  | encoder_4 | FCN        | 63.7 K\n",
      "9  | decoder_4 | FCN        | 78.8 K\n",
      "10 | encoder_5 | FCN        | 685 K \n",
      "11 | decoder_5 | FCN        | 722 K \n",
      "12 | encoder_6 | Sequential | 5.2 K \n",
      "13 | decoder_6 | FCN        | 5.6 K \n",
      "------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.404    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fc502d5db2449fabad2f35b35f0c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.09120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b48d7c00-f1d8-42e6-8e44-4da71e4444a8.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b48d7c00-f1d8-42e6-8e44-4da71e4444a8.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 75.3 K\n",
      "1  | decoder_0 | FCN        | 91.5 K\n",
      "2  | encoder_1 | FCN        | 218 K \n",
      "3  | decoder_1 | FCN        | 244 K \n",
      "4  | encoder_2 | FCN        | 114 K \n",
      "5  | decoder_2 | FCN        | 134 K \n",
      "6  | encoder_3 | FCN        | 72.3 K\n",
      "7  | decoder_3 | FCN        | 88.2 K\n",
      "8  | encoder_4 | FCN        | 63.7 K\n",
      "9  | decoder_4 | FCN        | 78.8 K\n",
      "10 | encoder_5 | FCN        | 685 K \n",
      "11 | decoder_5 | FCN        | 722 K \n",
      "12 | encoder_6 | Sequential | 5.2 K \n",
      "13 | decoder_6 | FCN        | 5.6 K \n",
      "------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.404    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddc8dcbb5354bc9a94f2282a7c70bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.025118864315095822\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b3bbe70f-341d-49f9-9c50-c4f7d73ae11e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_b3bbe70f-341d-49f9-9c50-c4f7d73ae11e.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 75.3 K\n",
      "1  | decoder_0 | FCN        | 91.5 K\n",
      "2  | encoder_1 | FCN        | 218 K \n",
      "3  | decoder_1 | FCN        | 244 K \n",
      "4  | encoder_2 | FCN        | 114 K \n",
      "5  | decoder_2 | FCN        | 134 K \n",
      "6  | encoder_3 | FCN        | 72.3 K\n",
      "7  | decoder_3 | FCN        | 88.2 K\n",
      "8  | encoder_4 | FCN        | 63.7 K\n",
      "9  | decoder_4 | FCN        | 78.8 K\n",
      "10 | encoder_5 | FCN        | 685 K \n",
      "11 | decoder_5 | FCN        | 722 K \n",
      "12 | encoder_6 | Sequential | 5.2 K \n",
      "13 | decoder_6 | FCN        | 5.6 K \n",
      "------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.404    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce1f46459284d5992bbae2874393b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.07585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_41eb966a-7f32-4c43-94b6-9bdb1becf2b4.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_41eb966a-7f32-4c43-94b6-9bdb1becf2b4.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 75.3 K\n",
      "1  | decoder_0 | FCN        | 91.5 K\n",
      "2  | encoder_1 | FCN        | 218 K \n",
      "3  | decoder_1 | FCN        | 244 K \n",
      "4  | encoder_2 | FCN        | 114 K \n",
      "5  | decoder_2 | FCN        | 134 K \n",
      "6  | encoder_3 | FCN        | 72.3 K\n",
      "7  | decoder_3 | FCN        | 88.2 K\n",
      "8  | encoder_4 | FCN        | 63.7 K\n",
      "9  | decoder_4 | FCN        | 78.8 K\n",
      "10 | encoder_5 | FCN        | 685 K \n",
      "11 | decoder_5 | FCN        | 722 K \n",
      "12 | encoder_6 | Sequential | 5.2 K \n",
      "13 | decoder_6 | FCN        | 5.6 K \n",
      "------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.404    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08905ef551f94b9fb5210d0f0890e854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.02089296130854041\n",
      "Restoring states from the checkpoint path at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_62b5b388-1f97-44f9-9a2d-6d59884d6bde.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/PycharmProjects/edc_mo_pdac_val/.lr_find_62b5b388-1f97-44f9-9a2d-6d59884d6bde.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type       | Params\n",
      "------------------------------------------\n",
      "0  | encoder_0 | FCN        | 75.3 K\n",
      "1  | decoder_0 | FCN        | 91.5 K\n",
      "2  | encoder_1 | FCN        | 218 K \n",
      "3  | decoder_1 | FCN        | 244 K \n",
      "4  | encoder_2 | FCN        | 114 K \n",
      "5  | decoder_2 | FCN        | 134 K \n",
      "6  | encoder_3 | FCN        | 72.3 K\n",
      "7  | decoder_3 | FCN        | 88.2 K\n",
      "8  | encoder_4 | FCN        | 63.7 K\n",
      "9  | decoder_4 | FCN        | 78.8 K\n",
      "10 | encoder_5 | FCN        | 685 K \n",
      "11 | decoder_5 | FCN        | 722 K \n",
      "12 | encoder_6 | Sequential | 5.2 K \n",
      "13 | decoder_6 | FCN        | 5.6 K \n",
      "------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.404    Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1872: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3.\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/edc_mo_pdac_val/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# %%capture --no-display\n",
    "\n",
    "func_objective = lambda trial: Optimization().objective(trial= trial, Xs= Xs, samples= samples, num_layers_option= [1,2,1], num_units_option= [2,6, 2],\n",
    "                                                        n_epochs_option= [20,100,20], lambda_option = [0.001, 1, 0.25], n_clusters_option= [2,4,1],\n",
    "                                                        latent_space_option = [32, 128, 32], batch_size=32,\n",
    "                                                        random_state=settings.RANDOM_STATE, n_jobs= 1)\n",
    "\n",
    "keep_trying = True\n",
    "while keep_trying:\n",
    "    try:\n",
    "        optimization_study = Optimization.optimize_optuna_and_save(study= optimization_study, n_trials = n_trials, date=date,\n",
    "                                                                   show_progress_bar= True, folder= settings.optimization_path, func= func_objective)\n",
    "        if new_study:\n",
    "            keep_trying = False\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483a7ff-d100-48b2-922c-ea061d085ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d20b5-9ab9-4279-9a4a-26d6f14ec482",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_optimization_history(optimization_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8537a-c234-4229-aeaf-4e35d58f016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_param_importances(optimization_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8bb0c5-d57c-494c-b953-73f86fbaa159",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_slice(optimization_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613bd30-1831-41d6-adb2-e1f8de8cddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_results = pd.read_csv(os.path.join(settings.optimization_path, f\"optimization_results_{date}.csv\"))\n",
    "best_trial = optimization_results.iloc[0]\n",
    "print(\"optimization_results.shape\", optimization_results.shape)\n",
    "optimization_results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
